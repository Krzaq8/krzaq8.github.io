[
    {
        "role": "system",
        "content": "You are a reward engineer trying to write reward functions to solve reinforcement learning tasks as effective as possible.\nYour goal is to write a reward function for the environment that will help the agent learn the task described in text. \nYour reward function should use useful variables from the environment as inputs.\nAs an example, the reward function signature can be: \n\ndef compute_reward(self):\n    env = self.env  # Do not skip this line. Afterwards, use env.{parameter_name} to access parameters of the environment.\n    ...\n    return reward, {}\nMake sure any new tensor or variable you introduce is on the same device as the input tensors. The output of the reward function should consist of two items:\n    (1) the total reward (tensor with reward per environment)\n    (2) a dictionary of each individual reward component (tensor with reward component per environment)\nThe code output should be formatted as a python code string: \"```python ... ```\".\n\nSome helpful tips for writing the reward function code:\n    (1) Make sure the type of each input variable is correctly specified; a float input variable should not be specified as torch.Tensor\n    (2) Most importantly, the reward code's input variables must contain only attributes of the provided environment class definition (namely, variables that have prefix self.). Under no circumstance can you introduce new input variables."
    },
    {
        "role": "user",
        "content": "The Python environment is class LeggedRobot(BaseTask):\n    \"\"\"Rest of environment ommitted\"\"\"\n\n    def _init_buffers(self):\n        # Buffer has shape (num_actors, 13). State for each actor root contains position([0:3]), rotation([3:7]), linear velocity([7:10]), and angular velocity([10:13]).\n        actor_root_state = self.gym.acquire_actor_root_state_tensor(self.sim)\n        # Buffer has shape (num_dofs, 2). Each DOF state contains position and velocity.\n        dof_state_tensor = self.gym.acquire_dof_state_tensor(self.sim)\n        # Buffer has shape (num_rigid_bodies, 3). Each contact force state contains one value for each X, Y, Z axis.\n        net_contact_forces = self.gym.acquire_net_contact_force_tensor(self.sim)\n        # Buffer has shape (num_rigid_bodies, 13). State for each rigid body contains position([0:3]), rotation([3:7]), linear velocity([7:10]), and angular velocity([10:13]).\n        rigid_body_state = self.gym.acquire_rigid_body_state_tensor(self.sim)\n        self.gym.refresh_dof_state_tensor(self.sim)\n        self.gym.refresh_actor_root_state_tensor(self.sim)\n        self.gym.refresh_net_contact_force_tensor(self.sim)\n        self.gym.refresh_rigid_body_state_tensor(self.sim)\n        self.gym.render_all_camera_sensors(self.sim)\n\n        # This is in the global frame\n        self.root_states = gymtorch.wrap_tensor(actor_root_state)\n        self.dof_state = gymtorch.wrap_tensor(dof_state_tensor)\n        self.net_contact_forces = gymtorch.wrap_tensor(net_contact_forces)\n        self.dof_pos = self.dof_state.view(self.num_envs, self.num_dof, 2)[..., 0]\n        self.dof_vel = self.dof_state.view(self.num_envs, self.num_dof, 2)[..., 1]\n        self.base_quat = self.root_states[:, 3:7]\n        self.rigid_body_state = gymtorch.wrap_tensor(rigid_body_state)\n        self.foot_velocities = self.rigid_body_state.view(self.num_envs, self.num_bodies, 13)[:, self.feet_indices, 7:10]\n\n        self.contact_forces = gymtorch.wrap_tensor(net_contact_forces).view(self.num_envs, -1, 3)\n\n        self.common_step_counter = 0\n        self.extras = {}\n\n        self.measured_heights = 0\n\n        self.noise_scale_vec = self._get_noise_scale_vec(self.cfg)\n        self.gravity_vec = to_torch(get_axis_params(-1., self.up_axis_idx), device=self.device).repeat( (self.num_envs, 1))\n        self.forward_vec = to_torch([1., 0., 0.], device=self.device).repeat((self.num_envs, 1))\n        self.torques = torch.zeros(self.num_envs, self.num_dof, dtype=torch.float, device=self.device, requires_grad=False)\n        self.p_gains = torch.zeros(self.num_dof, dtype=torch.float, device=self.device, requires_grad=False)\n        self.d_gains = torch.zeros(self.num_dof, dtype=torch.float, device=self.device, requires_grad=False)\n        self.actions = torch.zeros(self.num_envs, self.num_actions, dtype=torch.float, device=self.device, requires_grad=False)\n        self.last_actions = torch.zeros(self.num_envs, self.num_actions, dtype=torch.float, device=self.device, requires_grad=False)\n        self.last_dof_vel = torch.zeros_like(self.dof_vel)\n        self.last_root_vel = torch.zeros_like(self.root_states[:, 7:13])\n\n\n        self.feet_air_time = torch.zeros(self.num_envs, self.feet_indices.shape[0], dtype=torch.float,\n                                         device=self.device, requires_grad=False)\n        self.last_contacts = torch.zeros(self.num_envs, len(self.feet_indices), dtype=torch.bool, device=self.device,\n                                         requires_grad=False)\n        # These are in the local frame of the robot\n        self.base_lin_vel = quat_rotate_inverse(self.base_quat, self.root_states[:, 7:10])\n        self.base_ang_vel = quat_rotate_inverse(self.base_quat, self.root_states[:, 10:13])\n        # This is the downwards direction of the robot's base, used to measure orientation of the robot\n        self.projected_gravity = quat_rotate_inverse(self.base_quat, self.gravity_vec)\n\n        # joint positions offsets and PD gains\n        self.default_dof_pos = torch.zeros(self.num_dof, dtype=torch.float, device=self.device, requires_grad=False)\n        for i in range(self.num_dofs):\n            name = self.dof_names[i]\n            angle = self.cfg.init_state.default_joint_angles[name]\n            self.default_dof_pos[i] = angle\n            found = False\n            for dof_name in self.cfg.control.stiffness.keys():\n                if dof_name in name:\n                    self.p_gains[i] = self.cfg.control.stiffness[dof_name]\n                    self.d_gains[i] = self.cfg.control.damping[dof_name]\n                    found = True\n            if not found:\n                self.p_gains[i] = 0.\n                self.d_gains[i] = 0.\n                if self.cfg.control.control_type in [\"P\", \"V\"]:\n                    print(f\"PD gain of joint {name} were not defined, setting them to zero\")\n        self.default_dof_pos = self.default_dof_pos.unsqueeze(0)\n\ndef _process_dof_props(self, props, env_id):\n    if env_id == 0:\n        self.dof_pos_limits = torch.zeros(self.num_dof, 2, dtype=torch.float, device=self.device,\n                                            requires_grad=False)\n        self.dof_vel_limits = torch.zeros(self.num_dof, dtype=torch.float, device=self.device, requires_grad=False)\n        self.torque_limits = torch.zeros(self.num_dof, dtype=torch.float, device=self.device, requires_grad=False)\n        for i in range(len(props)):\n            self.dof_pos_limits[i, 0] = props[\"lower\"][i].item()\n            self.dof_pos_limits[i, 1] = props[\"upper\"][i].item()\n            self.dof_vel_limits[i] = props[\"velocity\"][i].item()\n            self.torque_limits[i] = props[\"effort\"][i].item()\n            # soft limits\n            m = (self.dof_pos_limits[i, 0] + self.dof_pos_limits[i, 1]) / 2\n            r = self.dof_pos_limits[i, 1] - self.dof_pos_limits[i, 0]\n            self.dof_pos_limits[i, 0] = m - 0.5 * r * self.cfg.rewards.soft_dof_pos_limit\n            self.dof_pos_limits[i, 1] = m + 0.5 * r * self.cfg.rewards.soft_dof_pos_limit\n\n    return props. Write a reward function for the following task: To make the go1 quadruped run forward with a velocity of exactly 2.0 m/s in the positive x direction of the global coordinate frame. The policy will be trained in simulation and deployed in the real world, so the policy should be as steady and stable as possible with minimal action rate. Specifically, as it's running, the torso should remain near a z position of 0.34, and the orientation should be perpendicular to gravity. Also, the legs should move smoothly and avoid the DOF limits.."
    },
    {
        "role": "assistant",
        "content": "Considering the policy feedback:\n\n1. **Task Score & Episode Length**: Task score and episode lengths indicate the policy is learning, with both metrics showing significant growth. The increase in episode length reflects an improvement in survivability, while the task score's growth suggests that the policy is better fulfilling the task requirements over time.\n\n2. **Action Smoothness**: The consistently large negative values for action smoothness indicate that the penalty might be too harsh or that the policy is struggling to achieve smooth actions. Given that smoothness is crucial for stability, especially in legged locomotion, adjusting its weight could offer a balance between encouraging smooth actions and not penalizing the policy too harshly.\n\n3. **DOF Limit Penalty**: The magnitude of the DOF limit penalty is relatively small compared to other components, but its presence is essential to avoid extreme joint positions. A slight adjustment to its weight could still offer a guideline to the policy without overshadowing other components.\n\n4. **Orientation Reward**: The orientation component demonstrates high values and less variance compared to other components, which suggests the policy adequately learns to maintain orientation. Given its consistently high values, its influence on the total reward is crucial but might need slight balancing adjustments.\n\n5. **Velocity & Z Position Rewards**: The high variance seen in the rewards for velocity and Z position suggests these are pivotal, challenging aspects the policy is learning to optimize. Both components are integral to the task, indicating the policy is actively learning to regulate speed and maintain a stable z position.\n\nBased on these insights, the following adjustments are proposed:\n\n- **Rebalance the weights** to ensure no single component excessively dominates the total reward.\n- **Adjust the scaling of action smoothness** to prevent overly harsh penalties, thus allowing more fluid movement without promoting jerky actions.\n- **Maintain a focus on critical task components**\u2014velocity and Z position\u2014since these are directly related to the primary task objectives.\n- **Fine-tuning the DOF limit penalty** considering its consistent low values, to ensure it guides behavior without unduly impacting the reward.\n\nImproved reward function:\n\n```python\ndef compute_reward(self):\n    env = self.env\n    \n    # Tweaked weights\n    velocity_weight = 1.0\n    z_pos_weight = 0.1\n    orientation_weight = 0.05\n    dof_limit_penalty_weight = 0.005\n    smoothness_weight = 0.005\n    \n    # Objectives\n    desired_velocity_x = 2.0\n    desired_z_pos = 0.34\n    desired_orientation = torch.tensor([0., 0., 0., 1.], device=env.device)\n\n    # Calculate errors\n    velocity_error = torch.abs(env.root_states[:, 7] - desired_velocity_x)\n    z_pos_error = torch.abs(env.root_states[:, 2] - desired_z_pos)\n    orientation_error = 1 - torch.abs(torch.sum(env.base_quat * desired_orientation, dim=1))\n    \n    dof_limit_penalty = torch.sum(torch.relu(torch.abs(env.dof_pos - env.default_dof_pos) - (env.dof_pos_limits[:, 1] - env.dof_pos_limits[:, 0]).unsqueeze(0)/2), dim=1)\n    \n    action_smoothness = torch.mean(torch.abs(env.actions - env.last_actions), dim=1)\n\n    # Rescale and compute scores\n    velocity_score = velocity_weight * torch.exp(-velocity_error)\n    z_pos_score = z_pos_weight * torch.exp(-z_pos_error)\n    orientation_score = orientation_weight * torch.exp(-orientation_error)\n    dof_limit_penalty_score = -dof_limit_penalty_weight * dof_limit_penalty\n    smoothness_score = -smoothness_weight * action_smoothness\n    \n    # Total reward composition\n    total_reward = velocity_score + z_pos_score + orientation_score + dof_limit_penalty_score + smoothness_score\n\n    # Components dictionary\n    reward_components = {\n        'velocity': velocity_score,\n        'z_pos': z_pos_score,\n        'orientation': orientation_score,\n        'dof_limit_penalty': dof_limit_penalty_score,\n        'action_smoothness': smoothness_score\n    }\n\n    return total_reward, reward_components\n```\n\nChanges Made:\n\n- **Exponential scoring** is used for velocity, z position, and orientation errors to soften the penalty for deviations and encourage precise control without penalizing the agent too harshly for small errors.\n- **Weight adjustments**: Reduced the weight of action smoothness and DOF limit penalties. This adjustment aims to balance the reward components more evenly, ensuring consistency in policy learning across all task aspects.\n- **Overall focus**: By rebalancing, the reward function aims to maintain a clear focus on the primary task objectives (velocity and z position) while ensuring the motion is smooth and within joint limits for robustness and real-world applicability."
    },
    {
        "role": "user",
        "content": "We trained a RL policy using the provided reward function code and tracked the values of the individual components in the reward function as well as global policy metrics such as task score and episode lengths after every 100 epochs and the maximum, mean, minimum values encountered:\ntask score: ['0.03', '5.97', '172.76', '385.48', '364.76', '387.85', '340.10', '322.11', '314.42', '340.65'], Max: 408.57, Mean: 273.84, Min: 0.01 \nepisode length: ['8.83', '623.75', '571.90', '901.90', '906.28', '890.53', '879.39', '849.23', '856.42', '879.49'], Max: 945.41, Mean: 775.22, Min: 8.83 \nrew action smoothness: ['-0.04', '-5.31', '-5.84', '-10.23', '-11.99', '-13.81', '-15.36', '-17.32', '-20.09', '-23.77'], Max: -0.04, Mean: -13.41, Min: -25.68 \nrew dof limit penalty: ['0.00', '-0.14', '-0.19', '-0.44', '-0.57', '-0.61', '-0.58', '-0.63', '-0.72', '-0.76'], Max: 0.00, Mean: -0.49, Min: -0.76 \nrew orientation: ['0.43', '27.78', '25.45', '40.70', '40.35', '39.96', '38.70', '37.00', '36.96', '38.45'], Max: 42.11, Mean: 34.18, Min: 0.43 \nrew total: ['3.17', '219.71', '346.45', '619.75', '600.91', '611.17', '563.67', '534.19', '524.96', '554.74'], Max: 649.26, Mean: 477.09, Min: 3.17 \nrew velocity: ['1.98', '139.96', '274.86', '508.18', '490.95', '505.06', '462.25', '438.96', '432.25', '461.93'], Max: 534.25, Mean: 386.76, Min: 1.98 \nrew z pos: ['0.80', '57.42', '52.17', '81.53', '82.17', '80.56', '78.67', '76.18', '76.57', '78.90'], Max: 85.62, Mean: 70.06, Min: 0.80 \nPlease carefully analyze the policy feedback and provide a new, improved reward function that can better solve the task. Some helpful tips for analyzing the policy feedback:\n    (1) If the task score is always near zero, then you must rewrite the entire reward function\n    (2) If the values for a certain reward component are near identical throughout, then this means RL is not able to optimize this component as it is written. You may consider\n        (a) Changing its scale or the value of its temperature parameter\n        (b) Re-writing the reward component \n        (c) Discarding the reward component\n    (3) If some reward components' magnitude is significantly larger, then you must re-scale its value to a proper range\nPlease analyze each existing reward component in the suggested manner above first, and then write the reward function code. The output of the reward function should consist of two items:\n    (1) the total reward (tensor with reward per environment)\n    (2) a dictionary of each individual reward component (tensor with reward component per environment)\nThe code output should be formatted as a python code string: \"```python ... ```\".\n\nSome helpful tips for writing the reward function code:\n    (1) Make sure the type of each input variable is correctly specified; a float input variable should not be specified as torch.Tensor\n    (2) Most importantly, the reward code's input variables must contain only attributes of the provided environment class definition (namely, variables that have prefix self.). Under no circumstance can you introduce new input variables."
    }
]